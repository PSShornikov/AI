# Отчет по лабораторной работе 
## по курсу "Искусственый интеллект"

## Нейросетям для распознавания изображений


### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|------------------------------------|--------------|
| Москаленко А. С. | Подготовил датасет, написал свою часть отчета |    3      |
| Дергач Д. К. | Обучил сверточную нейросеть, написал часть отчета |    3   |
| Шорников П. С. | Обучил однослойную и многослойную нейронные сети |   3   |

## Результат проверки

| Преподаватель     | Дата         |  Оценка       |
|-------------------|--------------|---------------|
| Сошников Д.В. |   6.11.2020           |      3         |

> *Опоздание в сдаче отчёта*

## Тема работы - Нейронные сети

Необходимо подготовить набор данных и построить несколько нейросетевых классификаторов: однослойная (многослойная) сеть и сверточная сеть для распознавания рукописных символов. В нашем 2-ом варианте - 5 первых символов греческого алфавита.

## Распределение работы в команде

Обязанности разбились так:

1. Москаленко - подготовка датасета, на основе подготовленных ранее всеми участника рукописных символов. На выходе 2 датасета с тренировочными и тестовыми данными, разбитые на единичные картинки.

2. Шорников - обучение однослойно и многослойной сети. На выходе обученная сеть и графики модели, точностей обучения.

3. Дергач - обучение сверточной сети. На выходе обученная сеть и графики модели, точностей обучения. Анализ полученных данных.

## Подготовка данных

Пример фотографии исходных листков с рукописными символами:
![Концептуализация](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/dergach.jpg)
![Концептуализация](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/moskalenko.jpg)
![Концептуализация](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/shornikov.jpg)


При подготовке данных нужно было изменить размер исходного снимка для равномерного разделения на фрагменты. Подробное описание подготовки длатасета можно увидеть в файле Script_split.ipynb.



![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/tree/master/ALL_Images/train)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/tree/master/ALL_Images/test)

## Загрузка данных

Для загрузки данны применялся класс ImageDataGenerator, который по указанному пути считывал изображения и разбевал их на batch по тензорам для всех классов. Датасеты организованны по условию (данные 2 участников - тренировочная выборка, а данные 3-го - тестовая выборка).

## Обучение нейросети

### Полносвязная однослойная сеть

![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/One_layer_model.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/One_layer_accurasy.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/One_layer_loss.png)

**Результаты**

Здесь помимо полносвязного слоя (Dense) были применен выравнивающий слой (Flatten). Как ни странно, но в этой модели результат тестовой выборки - 0,35, хоть и низкий, но максимальный на всех 3 моделях, что еще раз подтверждает несовершенность dataset, который описан в конечном выводе. Остальные параметры модели как batch_size и epochs брались со сверточной сети (так как она уже была закончена и подтвердила результатом на обучающей выборке).

### Полносвязная многослойная сеть

![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Multy_layer_model.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Multy_layer_accuracy.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Multy_layer_loss.png)

**Результаты**

Обучение многослойной модели также дало плохие результаты (но все равно это лучше чем сверточная модель) - 0,32. При создании модели были взяты почти те же слои (Faltten, Dropout, Batch_Normalization), а на место сверточных слоев пришли 3 полносвязных скрытых моделей. Стоит отметить, что эта модель стала раньше выдавать более высокие значения на тренировочной выборке, чем любая другая.

### Свёрточная сеть

![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Conv_model.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Conv_accuracy.png)
![](https://github.com/MAILabs-Edu-AI/lab-neural-networks-vision-valorand/blob/master/ALL_Images/Finally_Images/Conv_loss.png)

**Результаты**
Схема модели строилась с опорой на VGG19 (хотя число слоев и параметр depth уменьшенны). По общему шаблону использовалась схема активациия 'relu'. Слои: 18 сверточных (Convolution2D), 3 полносвязных (Dense), 1 выравнивающий (Flatten) и 1 Dropout (для исключений). Также стоит отметить использование Batch_normalization: без использования batch точность не превосходила 0,25 даже на тестовой выборке, когда с использованием показатели обучающей выборки координально изменились.

Однако и при примении batch не удалось получить хорошее значение для тестовой выборки (среднее значение - 0,29 , а максимальное ~ 0,5). При поиске возможностей улучшения моделей были попытки увеличения число сверточных слоев (до текущего), уменьшение batch_size с 64 до 32, и взято число epochs. Скорее всего проблема кроется в датасетах (описание этого будет в итоговом вывводе для всех моделей), ведь на тренировочной выборке были достигнуты очень хорошие результаты, а остальные изменения вели бы к еще большему переобучению модели.

## Выводы

В ходе данной лабораторной работу наши участники освоили полезный скрипт для разбития изображения на более мелкие, и конечно же обучению нейронных сетей.

Оказалось, что не так сложно написать модель, как подобрать наилучшие параметры, чтобы сеть правильно обучилась, и при этом не было переобучения (что у нас и получилось), и время обучения было приемлимым. Также мы убедились в важности подготовки датасета, скорее всего из-за него и получились такие низкие резултаты. Уже после более тщательного просмотра все изображений, стало понятно, что использование одного цвета ручки, более эталонная запись символов (особенно это пригодилось на греческом алфавите, в котором много кривых).

Возвращаясь ко времени обучения, были попытки взять обучение модели (пока не было Batch_normalization) количеством, что привело к огромным временным затратам, но не дало никакого результата, что еще раз подтверждает, что правильно подобранная по слоям модель будет эффективнее громоздкой неоптимизированной модели.

Также сложным этапом была организация командой работы, ведь каждый имеет свой график и необходимое время на выполения задания. Хотя мы старались "подгонять" друг друга и производить отчет о проделанной работе.
